{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:19:54.300360Z",
     "iopub.status.busy": "2024-12-11T14:19:54.299979Z",
     "iopub.status.idle": "2024-12-11T14:19:55.207597Z",
     "shell.execute_reply": "2024-12-11T14:19:55.206339Z",
     "shell.execute_reply.started": "2024-12-11T14:19:54.300327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:44:12.628747Z",
     "iopub.status.busy": "2024-12-11T11:44:12.627882Z",
     "iopub.status.idle": "2024-12-11T11:44:12.641608Z",
     "shell.execute_reply": "2024-12-11T11:44:12.640290Z",
     "shell.execute_reply.started": "2024-12-11T11:44:12.628704Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/diagt-for-train/output.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:18:59.161149Z",
     "iopub.status.busy": "2024-12-11T14:18:59.160748Z",
     "iopub.status.idle": "2024-12-11T14:18:59.176487Z",
     "shell.execute_reply": "2024-12-11T14:18:59.175380Z",
     "shell.execute_reply.started": "2024-12-11T14:18:59.161115Z"
    }
   },
   "outputs": [],
   "source": [
    "df['error_count'].max()\n",
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:17:51.669365Z",
     "iopub.status.busy": "2024-12-11T14:17:51.668991Z",
     "iopub.status.idle": "2024-12-11T14:17:59.026209Z",
     "shell.execute_reply": "2024-12-11T14:17:59.025118Z",
     "shell.execute_reply.started": "2024-12-11T14:17:51.669332Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "max_f1 = -1\n",
    "for i in range(df['error_count'].max()):\n",
    "    pred = df['error_count'].apply(lambda x: x < i)\n",
    "    f1 = f1_score(y, pred)\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        max_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:48:07.026674Z",
     "iopub.status.busy": "2024-12-11T11:48:07.026215Z",
     "iopub.status.idle": "2024-12-11T11:48:07.034468Z",
     "shell.execute_reply": "2024-12-11T11:48:07.032330Z",
     "shell.execute_reply.started": "2024-12-11T11:48:07.026629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776800665372886 11\n"
     ]
    }
   ],
   "source": [
    "print(max_f1, max_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:19:09.481383Z",
     "iopub.status.busy": "2024-12-11T14:19:09.481000Z",
     "iopub.status.idle": "2024-12-11T14:19:09.615655Z",
     "shell.execute_reply": "2024-12-11T14:19:09.614173Z",
     "shell.execute_reply.started": "2024-12-11T14:19:09.481347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.37      0.50     27370\n",
      "           1       0.67      0.92      0.78     37897\n",
      "\n",
      "    accuracy                           0.69     65267\n",
      "   macro avg       0.72      0.65      0.64     65267\n",
      "weighted avg       0.72      0.69      0.66     65267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = df['error_count'].apply(lambda x: x < 11)\n",
    "print(classification_report(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/sentences/train_v3_drcat_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:41:11.422071Z",
     "iopub.status.busy": "2024-12-11T12:41:11.420907Z",
     "iopub.status.idle": "2024-12-11T12:41:11.440118Z",
     "shell.execute_reply": "2024-12-11T12:41:11.438949Z",
     "shell.execute_reply.started": "2024-12-11T12:41:11.422025Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:41:12.248327Z",
     "iopub.status.busy": "2024-12-11T12:41:12.247924Z",
     "iopub.status.idle": "2024-12-11T12:41:30.474691Z",
     "shell.execute_reply": "2024-12-11T12:41:30.473319Z",
     "shell.execute_reply.started": "2024-12-11T12:41:12.248291Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:41:30.477690Z",
     "iopub.status.busy": "2024-12-11T12:41:30.477201Z",
     "iopub.status.idle": "2024-12-11T12:41:31.249551Z",
     "shell.execute_reply": "2024-12-11T12:41:31.248406Z",
     "shell.execute_reply.started": "2024-12-11T12:41:30.477636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90     20618\n",
      "           1       0.92      0.93      0.93     28331\n",
      "\n",
      "    accuracy                           0.91     48949\n",
      "   macro avg       0.91      0.91      0.91     48949\n",
      "weighted avg       0.91      0.91      0.91     48949\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      6752\n",
      "           1       0.92      0.93      0.93      9565\n",
      "\n",
      "    accuracy                           0.91     16317\n",
      "   macro avg       0.91      0.91      0.91     16317\n",
      "weighted avg       0.91      0.91      0.91     16317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train_tfidf)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:41:31.251429Z",
     "iopub.status.busy": "2024-12-11T12:41:31.251025Z",
     "iopub.status.idle": "2024-12-11T12:44:42.693901Z",
     "shell.execute_reply": "2024-12-11T12:44:42.689202Z",
     "shell.execute_reply.started": "2024-12-11T12:41:31.251393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     20618\n",
      "           1       0.98      0.97      0.98     28331\n",
      "\n",
      "    accuracy                           0.97     48949\n",
      "   macro avg       0.97      0.98      0.97     48949\n",
      "weighted avg       0.98      0.97      0.98     48949\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      6752\n",
      "           1       0.98      0.97      0.98      9565\n",
      "\n",
      "    accuracy                           0.97     16317\n",
      "   macro avg       0.97      0.97      0.97     16317\n",
      "weighted avg       0.97      0.97      0.97     16317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "X_train_tokens = [preprocess(text) for text in X_train]\n",
    "X_test_tokens = [preprocess(text) for text in X_test]\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_w2v_embedding(tokens, model, vector_size=100):\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "X_train_embedded = np.array([get_w2v_embedding(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_embedded = np.array([get_w2v_embedding(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "clf_logreg = LogisticRegression(max_iter=1000)\n",
    "clf_logreg.fit(X_train_embedded, y_train)\n",
    "\n",
    "\n",
    "y_train_logreg = clf_logreg.predict(X_train_embedded)\n",
    "print(classification_report(y_train, y_train_logreg))\n",
    "\n",
    "y_test_logreg = clf_logreg.predict(X_test_embedded)\n",
    "print(classification_report(y_test, y_test_logreg))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6243690,
     "sourceId": 10119179,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6263047,
     "sourceId": 10146272,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
