## Первый этап
Выполнен анализ необходимости применения методов машинного обучения для задачи классификации текстов. В обработанных датасетах отсутствуют очевидные признаки, которые позволили бы по факту своего наличия однозначно классифицировать тексты. Исключение - наличие квадратных скобок в домене написания писем (модель пытается создать шаблон, куда человек должен вписать адрес/имена).

## Второй этап
Проведено обучение модели логистической регрессии после как векторизации при помощи Count vectorizer, так и при помощи TfIDF на датасете kaggle_ai_gen_palm (эссе на две темы - письмо сенатору и обсуждение городов без авто). Обучение применялось как отдельно к различным доменам, так и вместе. Получены значения accuracy score (выборка сбалансированная), так и f1, близкие к единице. Предварительная версия состоит в наличии опечаток и пропусках пробелов между словами, что является почти однозначным маркером человеческого авторства. Кроме того, в EDA было показано, что средняя длина слова (как с лемматизацией, так и без нее) текстов, написанных моделями и людьми, значимо отличается, как и соотношение уникальности.

Аналогичные показатели (0.91 и более) показаны и для датасета OUTFOX для моделей ChatGPT(f1=0.99), flan t5(f1=0.91) и davinci(f1=0.98) при векторизации Tf-Idf. Также были проанализированы би- и триграммы как для мешка слов, так и для Tf-Idf для всех моделей датасета OUTFOX. Для Tf-Idf отмечается некоторое снижение качества предсказания f1 до 0.81 (ChatGPT), 0.74 (FLAN T5), 0.82 (davinci).

Учитывая размер датасета OUTFOX-chatgpt (30800 строк), были обучены две модели логистической регрессии (одна на данных после векторизации мешком слов, другая - Tf-Idf), которые вместе с векторизаторами (для консистентности) были загружены в pickle файлы с соответствующими названиями. Все модели обучены только для униграмм. Результаты применения предобученной модели:
 - для другого датасета (palm - 2762 строки) применение сопряжено с низким значение recall (0.14 BoW, 0.13 Tf-Idf) для сгененрированных ИИ текстов (высокая частота восприятия таких текстов как написанных человеком), accuracy 0.57. При снижении порога классификации до 0.01 отмечается повышение recall 0.53 при росте f1 до 0.74. Наиболее вероятной причиной является различие доменов (в OUTFOX их намного больше), что обуславливает необходимость учета соответствия доменов и тематик при обучении моделей.
 - для датасета из того же домена, человек + FLAN t5 применение предобученной модели также сопряжено со снижением recall для классификации в группу ИИ (0.36 для BoW и 0.38 для Tf-Idf), а также снижением f1 до 0.64. Это соотносится с несколько меньшей точностью моделей, обученных непосредственно на это датасете до accuracy 0.91, что может говорить, вероятно, о большем качестве данной модели в части мимикрии под человеческий стиль написания. Для модели, обученной на мешке слов также потребовалось опускать порог вплоть до 0.01, чтобы получить оптимальный f1: 0.81 (recall для класса ИИ 0.65). Для модели на Tf-Idf оптимальным является порог 0.15-0.2 (f1 0.74, recall 0.64).
 - для датасета человек + davinci применение предобученной модели имеет такой же уровень f1, как и при обучении самой chatgpt (около 1 для BoW и 0.95 для Tf-Idf). Манипуляций с порогом не требовалось.

Вывод - В целом модели классического машинного обучения относительно неплохо справляются с задачей бинарной классификации текстов на сгенерированные машиной и написанные человеком при условии соблюдения соответствия доменов.

## Третий этап
